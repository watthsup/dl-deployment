version: '3.8'

services:
  inference_server:
    hostname: "inference_server"
    image: nvcr.io/nvidia/tritonserver:22.11-py3
    ports:
      - '8997:8000'
      - '8998:8001'
      - '8999:8002'
    volumes:
      - ./model_repository:/models/
    command: tritonserver --model-repository=/models --model-control-mode=poll --repository-poll-secs 30
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  backend:
    build: ./backend
    volumes:
        - ./backend/:/app/

    ports:
      - 8000:8000

  frontend:
    build: ./frontend
    ports:
      - 8501:8501