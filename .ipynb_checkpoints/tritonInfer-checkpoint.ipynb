{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f5a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as tritonhttpclient\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import onnxruntime as rt\n",
    "import tritonclient.grpc as client\n",
    "import torch\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a33af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInferencer():\n",
    "    def __init__(self, model_name:str, engine:str = 'triton', url:str=None):\n",
    "        self.engine = engine\n",
    "        self.model_name = model_name\n",
    "        self.url = url\n",
    "        with open('imagenet-idx.txt') as f:\n",
    "            classes = [line.strip() for line in f.readlines()]\n",
    "        self.labels = classes\n",
    "        if self.engine == 'onnxruntime' or self.engine == 'onnxruntime-gpu':\n",
    "            sessOpt = rt.SessionOptions()\n",
    "            sessOpt.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "            sessOpt.add_session_config_entry(\"session.set_denormal_as_zero\", \"1\")\n",
    "            \n",
    "            if self.engine == 'onnxruntime':\n",
    "                providers=['CPUExecutionProvider']\n",
    "            else:\n",
    "                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "            self.inferencer = rt.InferenceSession(self.model_name, sessOpt, providers=providers)\n",
    "        elif self.engine == 'triton':\n",
    "            self.tritonConnector = client.InferenceServerClient(url=self.url)\n",
    "    \n",
    "    def infer(self, image_numpy):\n",
    "        if self.engine == 'onnxruntime' or self.engine == 'onnxruntime-gpu':\n",
    "            input_batch = self.preprocess(image_numpy)\n",
    "            onnx_output = self.inferencer.run(None, {'input' : input_batch})\n",
    "            onnx_output = torch.FloatTensor(np.array(onnx_output)).reshape(1,-1)\n",
    "            results = self.postprocess(onnx_output)\n",
    "        elif self.engine == 'triton':\n",
    "            input_batch = self.preprocess(image_numpy)\n",
    "            input0 = client.InferInput('input', (1, 3, 224, 224), \"FP16\")\n",
    "            input0.set_data_from_numpy(input_batch)\n",
    "            output = self.tritonConnector.infer(model_name=self.model_name, inputs=[input0])\n",
    "            logits = output.as_numpy('output') \n",
    "            results = self.postprocess(logits)\n",
    "        return results\n",
    "        \n",
    "    def preprocess(self, image_numpy):\n",
    "        transform = A.Compose(\n",
    "                [\n",
    "                    A.Resize(height=224, width=224),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                        max_pixel_value=255., \n",
    "                        p=1.0\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ])\n",
    "        transform = transform(image=image_numpy)\n",
    "        image = transform[\"image\"]\n",
    "        input_batch = image.unsqueeze(0).numpy()\n",
    "        \n",
    "        if self.engine == 'onnxruntime' or self.engine == 'onnxruntime-gpu':\n",
    "            input_batch = input_batch.astype(np.float32)\n",
    "        if self.engine == 'triton':\n",
    "            input_batch = input_batch.astype(np.float16)\n",
    "        return input_batch\n",
    "        \n",
    "    def postprocess(self, output, topK=10):\n",
    "        if self.engine == 'onnxruntime' or self.engine == 'onnxruntime-gpu':\n",
    "            onnx_output = output\n",
    "            onnx_output = torch.FloatTensor(np.array(onnx_output)).reshape(1,-1)\n",
    "            prob = torch.nn.functional.softmax(onnx_output, dim=1)[0] * 100\n",
    "            _, indices = torch.sort(prob, descending=True)\n",
    "            return [(self.labels[idx], prob[idx].item()) for idx in indices[:topK]]\n",
    "        elif self.engine == 'triton':\n",
    "            triton_output = output\n",
    "            logits = np.asarray(triton_output, dtype=np.float32)\n",
    "            probs = softmax(logits)\n",
    "            probs = np.argsort(probs).reshape(-1)[::-1] * 100\n",
    "            indices = np.argsort(logits).reshape(-1)[::-1] \n",
    "            zipped = list(zip(indices,probs))\n",
    "            #return [self.labels[idx] for idx in indices[:topK]]\n",
    "            return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6698e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images\\\\standard.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41def4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tritonInferencer = ModelInferencer(model_name='resnet18_trt', engine='triton', url='localhost:8001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04106fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before  uint8\n",
      "before  float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['standard poodle',\n",
       " 'Bedlington terrier',\n",
       " 'miniature poodle',\n",
       " 'toy poodle',\n",
       " 'Kerry blue terrier',\n",
       " 'komondor',\n",
       " 'Afghan hound, Afghan',\n",
       " 'Irish water spaniel',\n",
       " 'Lakeland terrier',\n",
       " 'soft-coated wheaten terrier']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = tritonInferencer.infer(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580f4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxInferencer = ModelInferencer(model_name='model_repository\\\\resnet18_trt\\\\1\\\\model.onnx', \n",
    "                                  engine='onnxruntime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65c63aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standard poodle', 73.5542221069336),\n",
       " ('Bedlington terrier', 22.600780487060547),\n",
       " ('miniature poodle', 2.38507342338562),\n",
       " ('toy poodle', 0.6931744813919067),\n",
       " ('Kerry blue terrier', 0.3209385871887207),\n",
       " ('komondor', 0.1573612540960312),\n",
       " ('Afghan hound, Afghan', 0.09467097371816635),\n",
       " ('Irish water spaniel', 0.08072715997695923),\n",
       " ('Lakeland terrier', 0.021591916680336),\n",
       " ('soft-coated wheaten terrier', 0.011789900250732899)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnxInferencer.infer(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6b092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a8a81ff5d755b15bad9772d7c59b5f150b144c443f5da213845dae9e79d687b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
